# -*- coding: utf-8 -*-
"""ANALISIS_SENTIMEN_ALGORITMA_SVM_ULASAN_APLIKASI_OXYGENSELFCARE.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1PMkHiPrr8QB9xsJ846CB-vOMHe_uw6c-
"""

!pip install google-play-scraper

from google_play_scraper import app, reviews
import pandas as pd
import datetime

from google_play_scraper import reviews, Sort

app_id = 'com.oxygen.selfcare'

def get_reviews(app_id, lang='id', count=1000, sort=Sort.NEWEST, filter_device_with=None, continuation_token=None, filter_score_with=None):
  try:
    result, continuation_token = reviews(
      app_id,
      lang=lang,
      country='id',
      sort=sort,
      count=count,
      filter_score_with=filter_score_with,
      filter_device_with=filter_device_with,
      continuation_token=continuation_token
    )

    return result, continuation_token
  except Exception as e:
    print("Error: ", e)
    return None, None



reviews, continuation_token = get_reviews(app_id)

if reviews is not None:
  print("Jumlah ulasan:", len(reviews))
  if len(reviews) > 0:
    print("Contoh ulasan:")
    print(reviews[0])

else:
  print("Tidak dapat mengambil ulasan")

import csv

def export_to_csv(reviews, file_name='hasil_scraper_ulasan_app_selfcareoxygen.csv'):
  if reviews:
    fieldnames =['Review ID','Username', 'Rating', 'Review Text', 'Date']

    with open(file_name, 'w', newline='', encoding='utf-8') as csvfile:
      writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
      writer.writeheader()

      for review in reviews:
        writer.writerow({
          'Review ID': review['reviewId'],
          'Username': review['userName'],
          'Rating': review['score'],
          'Review Text': review['content'],
          'Date': review['at']
        })

    print(f"Data berhasil diekspor ke '{file_name}'")
  else:
    print("Tidak ada data ulasan untuk diekspor.")

export_to_csv(reviews)

import pandas as pd

data= pd.read_csv('hasil_scraper_ulasan_app_selfcareoxygen.csv')
data.info()

data.head(5)

import pandas as pd

data= pd.read_csv('hasil_scraper_ulasan_app_selfcareoxygen.csv')
data.info()

data.head(2)

df = pd.DataFrame(data[['Date','Username', 'Rating', 'Review Text']])
df.head(5)

df.info()

df.drop_duplicates(subset = "Review Text", keep = 'first', inplace = True)

df.info()

df.head(2)

import pandas as pd
import numpy as np
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt

df['Review Text'] = df['Review Text'].fillna('')

text = ' '.join(df['Review Text'].astype(str).tolist())

stopwords = set(STOPWORDS)
stopwords.update(['https', 'co', 'RT', '...', 'amp'])

wc = WordCloud(stopwords=stopwords, background_color="black", max_words=500, width=800, height=400)

wc.generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()

import matplotlib.pyplot as plt
from collections import Counter

text = " ".join(df["Review Text"])

tokens = text.split()
word_counts = Counter(tokens)

top_words = word_counts.most_common(10)

word, count = zip(*top_words)

colors = plt.cm.tab10(range(len(word)))

plt.figure(figsize=(12, 6))
bars = plt.bar(word, count, color=colors)
plt.xlabel("Kata-Kata Sering Muncul", fontsize=12, fontweight='bold')
plt.ylabel("Jumlah Kata", fontsize=12, fontweight='bold')
plt.title("Frekuensi Kata", fontsize=14, fontweight='bold')
plt.xticks(rotation=45)

for bar, num in zip(bars, count):
  plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, num + 1, str(num), fontsize=12, color='black', ha='center')


plt.show()

import re
import string
import nltk

def remove_URL(tweet):
  if tweet is not None and isinstance(tweet, str):
    url = re.compile(r'https?://\S+|www\.\S+')
    return url.sub(r'', tweet)
  else:
    return tweet

def remove_html(tweet):
    if tweet is not None and isinstance(tweet, str):
      html = re.compile(r'<.*?>')
      return html.sub(r'', tweet)
    else:
      return tweet

def remove_emoji(tweet):
  if tweet is not None and isinstance(tweet, str):
     emoji_pattern = re.compile("["
          u"\U0001F600-\U0001F64F" # emoticons
          u"\U0001F300-\U0001F5FF" # symbols & pictographs
          u"\U0001F680-\U0001F6FF" # transport & map symbols
          u"\U0001F700-\U0001F77F" # alchemical symbols
          u"\U0001F780-\U0001F7FF" # Geometric Shapes Extended
          u"\U0001F800-\U0001F8FF" # Supplemental Arrows-C
          u"\U0001F900-\U0001F9FF" # Supplemental Symbols and Pictographs
          u"\U0001FA00-\U0001FA6F" # Chess Symbols
          u"\U0001FA70-\U0001FAFF" # Symbols and Pictographs Extended-A
          u"\U0001F004-\U0001F0CF" # Additional emoticons
          u"\U0001F1E0-\U0001F1FF" # flags
                             "]+", flags=re.UNICODE)
     return emoji_pattern.sub(r'', tweet) # This line was indented one level too deep
  else:
      return tweet

def remove_symbols(tweet):
  if tweet is not None and isinstance(tweet, str):
    tweet = re.sub(r'[^a-zA-Z0-9\s]', '', tweet)
    return tweet

def remove_numbers(tweet):
  if tweet is not None and isinstance(tweet, str):
    tweet = re.sub(r'\d', '', tweet)
    return tweet


df ['cleaning'] = df['Review Text'].apply(lambda x: remove_URL(x))
df ['cleaning'] = df['cleaning']. apply(lambda x: remove_html(x))
df ['cleaning'] = df['cleaning']. apply(lambda x: remove_emoji (x))
df ['cleaning'] = df['cleaning']. apply(lambda x: remove_symbols(x))
df ['cleaning'] = df['cleaning']. apply(lambda x: remove_numbers(x))

df.head(10)

def case_folding(text):
  if isinstance(text, str):
    lowercase_text = text.lower()
    return lowercase_text

  else:
    return text

df['case folding'] = df['cleaning'].apply(case_folding)
df.head(5)

import pandas as pd

# Fungsi penggantian kata tidak baku
def replace_taboo_words(text, kamus_tidak_baku):
    if isinstance(text, str):
        words = text.split()
        replaced_words = []
        kalimat_baku = []
        kata_diganti = []
        kata_tidak_baku_hash = []

        for word in words:
            if word in kamus_tidak_baku:
                baku_word = kamus_tidak_baku[word]
                if isinstance(baku_word, str) and all(char.isalpha() for char in baku_word):
                    replaced_words.append(baku_word)
                    kalimat_baku.append(baku_word)
                    kata_diganti.append(word)
                    kata_tidak_baku_hash.append(hash(word))
            else:
                replaced_words.append(word)

        replaced_text = ' '.join(replaced_words)

    else:
        replaced_text = ''
        kalimat_baku = []
        kata_diganti = []
        kata_tidak_baku_hash = []

    return replaced_text, kalimat_baku, kata_diganti, kata_tidak_baku_hash

data = pd.DataFrame(df[['Date','Username','Rating','Review Text','cleaning','case folding']])
data.head(5)

kamus_data = pd.read_excel("kamuskatabaku.xlsx")
kamus_tidak_baku = dict(zip(kamus_data['tidak_baku'], kamus_data['kata_baku']))

# Terapkan fungsi penggantian kata tidak baku
data['normalisasi'], data['kalimat_baku'], data['kata_diganti'], data['kata_tidak_baku_hash'] = zip(*data['case folding'].apply(lambda x: replace_taboo_words(x, kamus_tidak_baku)))
# data.head(100)

df = pd.DataFrame(data[['Date','Username','Rating','Review Text','cleaning','case folding','normalisasi']])

df.head(20)

def tokenize(text):
    tokens = text.split()
    return tokens

df['tokenize'] = df['normalisasi'].apply(tokenize)

df.head(5)

from nltk.corpus import stopwords
nltk.download('stopwords')
stop_words = stopwords.words('indonesian')

def remove_stopwords(text):
    return [word for word in text if word not in stop_words]

df['stopword removal'] = df['tokenize'].apply(lambda x: remove_stopwords(x))

df.head(5)

!pip install Sastrawi

from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from nltk.stem import PorterStemmer
from nltk.stem.snowball import SnowballStemmer

factory = StemmerFactory()
stemmer = factory.create_stemmer()

def stem_text(text):
    return [stemmer.stem(word) for word in text]

df['steming_data'] = df['stopword removal'].apply(lambda x: ' '.join(stem_text(x)))
df.head(5)

df.info()

df.to_csv('Hasil_Preprocessing_Data.csv',encoding='utf8', index=False);

import pandas as pd
import numpy as np
from PIL import Image
from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator
import matplotlib.pyplot as plt



# Access the 'stemming_data' column from the 'data' DataFrame
text = ' '.join(df['steming_data'].astype(str).tolist())

stopwords = set(STOPWORDS)
stopwords.update(['https', 'co', 'RT', '...', 'amp', 'ya'])

wc = WordCloud(stopwords=stopwords, background_color="black", max_words=500, width=800, height=400)

wc.generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wc, interpolation='bilinear')
plt.axis("off")
plt.show()

import matplotlib.pyplot as plt
from collections import Counter

text = " ".join(df["steming_data"])

tokens = text.split()
word_counts = Counter(tokens)

top_words = word_counts.most_common(10)

word, count = zip(*top_words)


# Definisikan palet warna
colors = plt.cm.tab10(range(len(word)))

plt.figure(figsize=(12, 6))
bars = plt.bar(word, count, color=colors)
plt.xlabel("Kata-Kata Sering Muncul", fontsize=12, fontweight='bold')
plt.ylabel("Jumlah Kata", fontsize=12, fontweight='bold')
plt.title("Frekuensi Kata", fontsize=18, fontweight='bold')
plt.xticks(rotation=45)

# Menambahkan angka rata tengah di atas setiap bar
for bar, num in zip(bars, count):
  plt.text(bar.get_x() + bar.get_width() / 2 - 0.1, num + 1, str(num), fontsize=12, color='black', ha='center')

plt.show()

import pandas as pd

data = pd.read_csv('Hasil_Preprocessing_Data.csv')
data.head(5)

data.info()

df = data.dropna()

df.info()

import pandas as pd

def determine_sentiment(text):
  positive_count = sum(1 for word in text.split() if word in positive_lexicon)
  negative_count = sum(1 for word in text.split() if word in negative_lexicon)
  if positive_count > negative_count:
    return "Positif"
  elif positive_count < negative_count:
    return "Negatif"

positive_lexicon = set(pd.read_csv("positive.tsv", sep="\t", header=None) [0])
negative_lexicon = set(pd.read_csv("negative.tsv", sep="\t", header=None) [0])

def replace_none_sentiment(sentiments):
  replace_flag = "Positif"
  for i in range(len(sentiments)):
    if sentiments[i] is None:
      sentiments[i] = replace_flag
      replace_flag = "Negatif" if replace_flag == "Positif" else "Positif"
  return sentiments

df ['Sentiment' ] = df ['steming_data' ].apply(determine_sentiment)
df ['Sentiment' ] = replace_none_sentiment(df['Sentiment' ].tolist())

df.head( )

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

sentiment_count = df[ 'Sentiment' ]. value_counts ()
sns.set_style('whitegrid')

fig, ax = plt.subplots(figsize=(6, 4))
ax = sns.barplot(x=sentiment_count. index, y=sentiment_count.values, palette='pastel')
plt.title('Jumlah Analisis Sentimen', fontsize=14, pad=20)
plt.xlabel('Class Sentiment', fontsize=12)
plt.ylabel('Jumlah Tweet', fontsize=12)

total = len(df['Sentiment'])

for i, count in enumerate(sentiment_count.values):
  percentage = f'{100 * count / total:.2f}%'
  ax.text(i, count + 0.10, f'{count}\n({percentage})', ha='center', va='bottom')

plt. show()

df.to_csv('Hasil_Labelling_Data.csv', encoding='utf8', index=False)

import pandas as pd

data = pd.read_csv('Hasil_Labelling_Data.csv')
data.head()

data.info()

df = data.dropna()

df.info()

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(df['steming_data'], df['Sentiment'], test_size=0.2, random_state=42)

train_set = pd.DataFrame({'text': X_train, 'sentiment' : y_train})
train_set.to_csv('train_data.csv', index=False)

test_set = pd.DataFrame({'text': X_test, 'sentiment' : y_test})
test_set.to_csv('test_data.csv', index=False)

print(f'Jumlah Data Latih: {len(X_train)}')
print(f'Jumlah Data Uji: {len(X_test)}')

import matplotlib.pyplot as plt

train_size = len(X_train)
test_size = len(X_test)

plt.figure(figsize=(8, 6))
bars = plt.bar(['Data Latih', 'Data Uji'], [train_size, test_size], color=['blue', 'orange'])

for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2, height + 0.7, f'{height} ({height / (train_size + test_size) * 100:.2f}%)',
             ha='center', va='bottom')

plt.title('Jumlah Data Latih dan Data Uji')
plt.xlabel('Jenis Data')
plt.ylabel('Jumlah Data')
plt.show()

from sklearn.svm import SVC
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
X_train_vectorized = vectorizer.fit_transform(X_train)
X_test_vectorized = vectorizer.transform(X_test)

print("Matriks Vektorisasi untuk Data Latih: ")
print(X_train_vectorized.toarray())

print("\nSebagian kecil Matriks Vektorisasi untuk Data Latih: ")
print(X_train_vectorized[:5, :].toarray())

svm = SVC(kernel='linear')
svm.fit(X_train_vectorized, y_train)

y_pred_svm = svm.predict(X_test_vectorized)

cm_svm = confusion_matrix(y_test, y_pred_svm)
print("SVM Confusion Matrix:")
print(cm_svm)

import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(model_name, y_true, y_pred):
    cm = confusion_matrix(y_true, y_pred)

    plt.figure(figsize=(6,6))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
                xticklabels=['positif', 'negatif'],
                yticklabels=['positif', 'negatif'])

    plt.title(f"{model_name} Confusion Matrix")
    plt.xlabel("Predicted Label")
    plt.ylabel("True Label")
    plt.show()

# Example usage: Ensure y_test and y_pred_svm are already defined
plot_confusion_matrix("SVM", y_test, y_pred_svm)

accuracy_svm = accuracy_score(y_test, y_pred_svm)
print("SVM Accuracy:", accuracy_svm)
print("")

accuracy_svm_percentage = accuracy_svm * 100
print("SVM Accuaracy:", "{:.2f}%".format(accuracy_svm_percentage))
print("")

print("SVM Classification Report:")
print(classification_report(y_test, y_pred_svm))

import pandas as pd

data = pd.read_csv("Hasil_Labelling_Data.csv")
data.head()

import pandas as pd
from wordcloud import WordCloud
import matplotlib.pyplot as plt

sentimen_negatif = data[data['Sentiment' ] == 'Negatif'] ['steming_data']. str.cat(sep=' ')
sentimen_positif = data[data['Sentiment' ] == 'Positif']['steming_data'].str.cat(sep=' ')

def create_wordcloud(text, title):
  wordcloud = WordCloud(width=800, height=400, random_state=42, max_font_size=100, background_color='black').generate(text)

  plt.figure(figsize=(10, 5))
  plt.imshow(wordcloud, interpolation='bilinear')
  plt.axis('off')
  plt.title(title)
  plt.show()

create_wordcloud(sentimen_negatif, 'WordCloud Sentimen Negatif')

create_wordcloud(sentimen_positif, 'WordCloud Sentimen Positif')

text = ' '.join(data['steming_data'].apply(lambda x: str(x) if isinstance(x, (str, int, float)) else''))
wordcloud = WordCloud(width=800, height=400, background_color='black').generate(text)

plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')

plt.axis("off")
plt.show()

import pandas as pd

data = pd.read_csv('Hasil_Labelling_Data.csv')
data.info()

import matplotlib.pyplot as plt

rating_counts = data['Rating'].value_counts()
rating_counts = rating_counts.sort_index()

colors = ['red', 'lightcoral', 'lightgreen', 'lightsalmon', 'lightblue']

plt.figure(figsize=(8,6))
bars = plt.bar(rating_counts.index, rating_counts.values, color=colors)
plt.title('Jumlah Rating', fontsize=14, fontweight='bold')
plt.xlabel('Rating/Score')
plt.ylabel('Jumlah')
plt.xticks(rating_counts.index)

for bar in bars:
  height = bar.get_height()
  plt.text(bar.get_x() + bar.get_width() / 2, height, str(int(height)), ha='center', va='bottom')

plt. show()